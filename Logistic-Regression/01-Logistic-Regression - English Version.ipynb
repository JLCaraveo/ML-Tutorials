{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ace67d3",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc42578",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Logistic regression is a type of regression analysis used to predict the result of a categorical variable, that is, used to classify a data set according to the possible categories given by the variable to be predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aad4232",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe033b52",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The algorithm predicts the probability that a certain example belongs to a certain category. But, for each case study, an approval threshold have to be specified, it is a number from 0 to 1, given by the user, who will determine, if the probability is greater than this threshold, then this example belongs to a certain category.\n",
    "\n",
    "Having this clear, our hypothesis is as follows:\n",
    "\n",
    "We are going to use a threshold of 0.5, therefore:\n",
    "\n",
    "* If $h_0(x) \\ge 0.5$, the prediction will be \"$y=1$\"\n",
    "* If $h_0(x) \\le 0.5$, the prediction will be \"$y=0$\"\n",
    "\n",
    "Note that $0 \\le h_0 \\le 1$.\n",
    "\n",
    "And how to get $h_0$:\n",
    "\n",
    "$$h_0(x) = g(\\theta_0 + \\theta_1 \\cdot x_1 + \\theta_2 \\cdot x_2 + \\dots + \\theta_n \\cdot x_n) = g(z)$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $g(z) = \\frac{1}{1 + e^{-\\theta^T \\cdot X}}$\n",
    "\n",
    "Once we analyze the formulas. First, the value of $\\theta^T \\cdot X$ is obtained and a function is applied to this result which can convert the result of $\\theta^T \\cdot X$ to one that ranges from 0 to 1, and thus, given the threshold, we can decide if we can classify that example as \"$y=1$\" or \"$y=0$\".\n",
    "\n",
    "This function is called the sigmoid function:\n",
    "\n",
    "$$Sigmoid = \\frac{1}{1 + e^{-X}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbc6ffa",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ffd08",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The cost function for a linear regression is as follows:\n",
    "\n",
    "$$J(\\theta_0,\\ \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^{m}(h_0(x^i) - y^i)^2$$\n",
    "\n",
    "For logistic regression we cannot use this one, since this will result in a non-convex function, so it would never be possible to find an optimal global minimum to minimize it.\n",
    "\n",
    "We have to modify this function in order to work with the two possible values \"$y=1$\" and \"$y=0$\". Our cost function would look like this:\n",
    "\n",
    "$$\n",
    "Cost(h_0(x), y) = \\left \\{\n",
    "\\begin{array}{ll}\n",
    "-log(h_0(x)) & if\\ y=1 \\\\\n",
    "-log(1 - h_0(x)) & if\\ y=0\n",
    "\\end{array}\n",
    "\\right .\n",
    "$$\n",
    "\n",
    "But the above may be difficult to understand, for this we simplify the function as follows:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m Cost(h_0(x_i), y_i)$$\n",
    "\n",
    "Taking as reference the equations for \"$y=1$\" and \"$y=0$\", we have a formula where we can work with the two possible results:\n",
    "\n",
    "$$J(\\theta) = - \\frac{1}{m} \\left [ \\sum_{i=1}^m y_i log(h_0(x_i)) + (1-y_i)log(1 - h_0(x_i )) \\right]$$\n",
    "\n",
    "To make the prediction of a new element $x$:\n",
    "\n",
    "$$Output\\ h_0(x) = \\frac{1}{1+e^{-\\theta^T x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4093472",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98131b9c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How do we get the optimal $\\theta$?\n",
    "\n",
    "As in the linear regression algorithm, we will use gradient descent to help us find these parameters. The algorithm is as follows:\n",
    "\n",
    "Repeat until converge {\n",
    "     $$\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}(h_0(x^i) - y^i)\\cdot x^i_j$$\n",
    "\n",
    "}\n",
    "\n",
    "where:\n",
    "* $\\alpha$ is the learning rate.\n",
    "\n",
    "We notice that the algorithm looks identical to the linear regression algorithm, but we must make something clear, now, the function to calculate $h_0(x)$ is different, you have to use:\n",
    "\n",
    "$$h_0(x) = \\frac{1}{1+e^{-\\theta^T x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f11b11",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Code Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357da5f4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An example dataset that helps us understand how the logistic regression algorithm is used is the dataset for predicting malignant or benign tumors based on certain characteristics.\n",
    "\n",
    "The dataset that will be used provides us with characteristics of the tumors such as\n",
    "\n",
    "* identification\n",
    "* diagnosis\n",
    "* average_radius\n",
    "* medium_texture\n",
    "* mean_perimeter\n",
    "* mean_area\n",
    "* medium_smooth\n",
    "* medium_compactness\n",
    "* half_concavity\n",
    "* concave_mean points\n",
    "* mean_symmetry\n",
    "\n",
    "Among other important information. For this, we will try to predict the diagnosis (M = Malignant, B = Benign) according to the specifications of each tumor.\n",
    "\n",
    "The data is found in the following link: https://www.kaggle.com/yasserh/breast-cancer-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4a7e67",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5070a4bf",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/breast-cancer.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261e9a74",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have 569 examples and 30 characteristics, we do not take into account the ID, nor the diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "379dbd05",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212, 357)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['diagnosis'][data['diagnosis'] == 'M'].count(), \\\n",
    "data['diagnosis'][data['diagnosis'] == 'B'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edaa539",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The dataset contains 212 cases of malignant tumors and 357 cases of benign tumors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1c100c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When a regression algorithm is going to be applied, it is important to verify that the data which it is going to work is numerical. Otherwise, some transformation work will have to be done to help us convert the categorical variables to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "79b8aae6",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 32 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   id                       569 non-null    int64  \n",
      " 1   diagnosis                569 non-null    object \n",
      " 2   radius_mean              569 non-null    float64\n",
      " 3   texture_mean             569 non-null    float64\n",
      " 4   perimeter_mean           569 non-null    float64\n",
      " 5   area_mean                569 non-null    float64\n",
      " 6   smoothness_mean          569 non-null    float64\n",
      " 7   compactness_mean         569 non-null    float64\n",
      " 8   concavity_mean           569 non-null    float64\n",
      " 9   concave points_mean      569 non-null    float64\n",
      " 10  symmetry_mean            569 non-null    float64\n",
      " 11  fractal_dimension_mean   569 non-null    float64\n",
      " 12  radius_se                569 non-null    float64\n",
      " 13  texture_se               569 non-null    float64\n",
      " 14  perimeter_se             569 non-null    float64\n",
      " 15  area_se                  569 non-null    float64\n",
      " 16  smoothness_se            569 non-null    float64\n",
      " 17  compactness_se           569 non-null    float64\n",
      " 18  concavity_se             569 non-null    float64\n",
      " 19  concave points_se        569 non-null    float64\n",
      " 20  symmetry_se              569 non-null    float64\n",
      " 21  fractal_dimension_se     569 non-null    float64\n",
      " 22  radius_worst             569 non-null    float64\n",
      " 23  texture_worst            569 non-null    float64\n",
      " 24  perimeter_worst          569 non-null    float64\n",
      " 25  area_worst               569 non-null    float64\n",
      " 26  smoothness_worst         569 non-null    float64\n",
      " 27  compactness_worst        569 non-null    float64\n",
      " 28  concavity_worst          569 non-null    float64\n",
      " 29  concave points_worst     569 non-null    float64\n",
      " 30  symmetry_worst           569 non-null    float64\n",
      " 31  fractal_dimension_worst  569 non-null    float64\n",
      "dtypes: float64(30), int64(1), object(1)\n",
      "memory usage: 142.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d685d8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "All of our data is numeric, so no transformation is necessary before applying the algorithm, also  we do not have null data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c83799a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Variables to be used for the algorithm\n",
    "X = data.iloc[:, 2:]\n",
    "m = len(X)\n",
    "y = data['diagnosis']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b873db",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The variable to predict is categorical, so it has to be transformed to numeric. Normally an encoding technique is used, but in this case study we have two possible outcomes, therefore:\n",
    "\n",
    "* $y=1$ when the diagnosis is \"M\".\n",
    "* $y=0$ when the diagnosis is \"B\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f44bd7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y = y.apply(lambda x: 0 if x == 'B' else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b37fc7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order to handle a vectorized solution, it is necessary to add a column of ones at the beginning of the X variables, this helps us that $\\theta_0$ is not modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e24725ae",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ones = [1] * len(X)\n",
    "\n",
    "X.insert(0, 'ones', ones)\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b002fa4f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "329ccb8c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.e**(-x))\n",
    "\n",
    "# Getting Predictions\n",
    "def get_y_pred(x, thetas):\n",
    "    return sigmoid(x.dot(thetas.T))\n",
    "\n",
    "# Getting the cost\n",
    "def get_cost(y, y_pred):\n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        cost += (y[i] * np.log(y_pred[i])) + ((1-y[i]) * np.log(1-y_pred[i]))\n",
    "    return -1 * (1/m) * cost\n",
    "\n",
    "# Gradient Descent\n",
    "def get_gradient(x, y, n_iter, thetas, alpha=0.01):\n",
    "    for i in range(n_iter):\n",
    "        y_pred = get_y_pred(x, thetas)\n",
    "        \n",
    "        thetas = thetas - (alpha * ((1 / m)*((y_pred - y).T.dot(x))))\n",
    "        \n",
    "    return thetas\n",
    "\n",
    "\n",
    "# Optimal theta\n",
    "initial_thetas = np.zeros([X.shape[1]]).T\n",
    "n_iter = 100000\n",
    "thetas = get_gradient(X, y, n_iter, initial_thetas)\n",
    "\n",
    "# Obtaining the predictions according to the optimal theta.\n",
    "y_pred = get_y_pred(X, thetas)\n",
    "\n",
    "# Get 0 or 1 according to the threshold.\n",
    "threshold = 0.5\n",
    "y_pred2 = [1 if pred > threshold else 0 for pred in y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be56bca",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The best way to check how well our model is predicting is to use a confusion matrix, this helps us check for true positives, true negatives, false positives and false negatives.\n",
    "\n",
    "The Sklearn library provides us with a function to obtain this matrix. Which results in an array as follows:\n",
    "\n",
    "$$\\begin{pmatrix} True\\ Positives & False\\ Positives \\\\ False\\ Negatives & True\\ Negatives \\end{pmatrix}$$\n",
    "\n",
    "This matrix also helps us calculate:\n",
    "\n",
    "* Accuracy: Of the predictions, what is the proportion that the algorithm predicts correctly.\n",
    "\n",
    "$$Accuracy = \\frac{True\\ Positives\\ +\\ True\\ Negatives}{m}$$\n",
    "\n",
    "* Precision: Of the total number of positive predictions, what proportion is actually true positive.\n",
    "\n",
    "$$Precision = \\frac{True\\ Positives}{True\\ Positives\\ +\\ False\\ Positives}$$\n",
    "\n",
    "* Recall: Of the total number of positive predictions, what proportion actually predicts as a true positive.\n",
    "\n",
    "$$Recall = \\frac{True\\ Positives}{True\\ Positives\\ +\\ False\\ Negatives}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1e470d2",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9121265377855887, Presicion: 0.9971988795518207, Recall: 0.8790123456790123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(y, y_pred2)\n",
    "\n",
    "VP = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1]\n",
    "VN = conf_matrix[1][1]\n",
    "FN = conf_matrix[1][0]\n",
    "\n",
    "accuracy = (VP + VN) / m\n",
    "presicion = VP / (VP + FP)\n",
    "recall = VP / (VP + FN)\n",
    "\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}'.format(accuracy, presicion, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0353bc59",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our algorithm obtains an accuracy of almost 100%, with high precision and recall. Therefore, it is concluded that this algorithm helps to predict with almost 100% accuracy a malignant or benign tumor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d989ed9",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Algorithm with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be3661cd",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9121265377855887, Precision: 0.9971988795518207, Recall: 0.8790123456790123\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = data.iloc[:, 2:].values\n",
    "m = len(X)\n",
    "y = data['diagnosis']\n",
    "\n",
    "# Converting categorical data to numeric\n",
    "y = y.apply(lambda x: 0 if x == 'B' else 1)\n",
    "\n",
    "# Training the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Getting predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y, y_pred2)\n",
    "\n",
    "VP = conf_matrix[0][0]\n",
    "FP = conf_matrix[0][1]\n",
    "VN = conf_matrix[1][1]\n",
    "FN = conf_matrix[1][0]\n",
    "\n",
    "accuracy = (VP + VN) / m\n",
    "presicion = VP / (VP + FP)\n",
    "recall = VP / (VP + FN)\n",
    "\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}'.format(accuracy, presicion, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52029b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can see how we obtain the same results, but with the advantage that now the procedure is applied with a few lines of code."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "José Luis Higuera Caraveo"
   }
  ],
  "colab": {
   "collapsed_sections": [
    "uW-Xfqevr7Sy"
   ],
   "name": "Recursos Humanos",
   "provenance": []
  },
  "date": "Febrero 28 2022",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "title": "Logistic Regression with Python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
